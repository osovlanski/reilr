{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"img/eps-greedy.png\">\n",
    "<img src=\"img/algo.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_type = \"FrozenLake8x8-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_type)\n",
    "\n",
    "alpha = 0.05\n",
    "max_steps = 250 \n",
    "episodes = 500000\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon = 1.0\n",
    "gamma = 0.95\n",
    "_lambda = 0.0\n",
    "#decay_factor = 0.999985\n",
    "#decay_factor = 0.00005\n",
    "\n",
    "NUMBER_OF_EVAL_SIMS = 100\n",
    "history = []\n",
    "is_decay = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Initialize_Q():\n",
    "    Q = np.zeros((env.env.nS,env.env.nA))\n",
    "    return Q\n",
    "\n",
    "def Initialize_E():\n",
    "    E = np.zeros((env.env.nS,env.env.nA))\n",
    "    return E\n",
    "\n",
    "def Initalize_Policy(env):\n",
    "    Policy = np.array([env.action_space.sample() for _ in range(env.env.nS)])\n",
    "    return Policy\n",
    "\n",
    "def tune_params():\n",
    "    global epsilon\n",
    "    epsilon = epsilon * 0.999985\n",
    "    \n",
    "def tune_params2(episode):\n",
    "    global epsilon\n",
    "    epsilon = epsilon_min + (epsilon_max - epsilon_min)*np.exp(-0.00005*episode)\n",
    "\n",
    "def eps_greedy_policy(state,Q):    \n",
    "    if random.uniform(0,1) <  epsilon:\n",
    "        new_action = env.action_space.sample() #explore\n",
    "    else:\n",
    "        new_action = np.argmax(Q[state,:]) #exploit\n",
    "        \n",
    "    return new_action\n",
    "\n",
    "def policy_eval(policy):\n",
    "    \"\"\"\n",
    "    policy should be an iterable with length of number of states (action per state)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for i in range(NUMBER_OF_EVAL_SIMS):\n",
    "        state = env.reset()\n",
    "\n",
    "        run_reward = 0\n",
    "        is_done = False\n",
    "        while not is_done:\n",
    "            state, reward, is_done, _ = env.step(policy[state])\n",
    "\n",
    "            run_reward += reward\n",
    "\n",
    "        rewards.append(run_reward)\n",
    "\n",
    "    return np.mean(rewards)\n",
    "\n",
    "hl,=plt.plot(np.array([]),np.array([]))\n",
    "\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "def updateLine(x,y,print_to_screen=False):\n",
    "    hl.set_xdata(np.append(hl.get_xdata(), x))\n",
    "    hl.set_ydata(np.append(hl.get_ydata(), y))\n",
    "    if print_to_screen:\n",
    "        plt.plot(hl.get_xdata(),hl.get_ydata())\n",
    "        display.clear_output(wait=True) \n",
    "        display.display(plt.gcf())\n",
    "        display.display(\"x:{0} y:{1:.3f} alpha:{2:.3f} lambda:{3:.3f} starting epsilon:{4:.3f} current epsilon:{5:.3f}\"\n",
    "        .format(x,y,alpha,_lambda,epsilon_max,epsilon))\n",
    "        time.sleep(0.15)\n",
    "        \n",
    "def Sarsa_lambda(episodes=episodes,max_steps=max_steps,is_decay=True):\n",
    "    Q = Initialize_Q()\n",
    "    total_steps = 0\n",
    "    rewards = []\n",
    "    updateLine(0,0)\n",
    "\n",
    "    for k in range(episodes):\n",
    "        \n",
    "        #init E,S,A\n",
    "        E = Initialize_E()\n",
    "        state = env.reset() #random.randint(0,env.env.nS-1) \n",
    "        action = eps_greedy_policy(state,Q)\n",
    "        #R = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            #Take action A, ovserve R,S'\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            new_action = eps_greedy_policy(new_state,Q)\n",
    "            \n",
    "            delta_error = reward + gamma*Q[new_state,new_action]-Q[state,action]\n",
    "            E[state,action]+=1\n",
    "            Q = np.add(Q, np.multiply(alpha * delta_error, E))\n",
    "            E = np.multiply(gamma * _lambda, E)\n",
    "            \n",
    "            state=new_state\n",
    "            action=new_action\n",
    "            #R+=reward\n",
    "            total_steps+=1\n",
    "\n",
    "            if  total_steps % 1000 == 0:\n",
    "                policy = np.argmax(Q,axis=1)\n",
    "                policy_evaluate = policy_eval(policy)\n",
    "                updateLine(total_steps,policy_evaluate,True)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        #print(\"is_decay\",is_decay)\n",
    "        if is_decay:\n",
    "            tune_params2(k)\n",
    "\n",
    "        if total_steps > 1e6:\n",
    "            break    \n",
    "        #rewards.append(R)\n",
    "         \n",
    "    return Q,np.argmax(Q,axis=1) # returns Q and the policy\n",
    "\n",
    "Q,Policy = Sarsa_lambda(episodes,max_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hl.get_xdata(),hl.get_ydata()) \n",
    "plt.savefig(\"policy_eval_over_steps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = np.array(history)\n",
    "\n",
    "# plt.plot(history[:, 0], history[:, 1], '.') \n",
    "# plt.xlabel('steps:')\n",
    "# plt.ylabel('Cumulative rewards')\n",
    "\n",
    "# str_epsilon = epsilon\n",
    "# if is_decay:\n",
    "#     str_epsilon = str(epsilon_origin)+\"*0.99^n\"\n",
    "\n",
    "# title = \"epsilon: {}, alpha: {}, lambda: {}\".format(str_epsilon, alpha, _lambda)\n",
    "# plt.title(title)\n",
    "# plt.savefig(\"policy_eval_over_steps.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_action(action):\n",
    "    if action == 0:\n",
    "        return \"<\"\n",
    "    if action == 1:\n",
    "        return \"v\"\n",
    "    if action == 2:\n",
    "        return \">\"\n",
    "    if action == 3:\n",
    "        return \"^\"\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "\n",
    "if env_type == \"FrozenLake8x8-v0\":\n",
    "    print(np.array([decode_action(action) for action in Policy]).reshape((8, 8)))\n",
    "if env_type == \"FrozenLake-v0\":\n",
    "    print(np.array([decode_action(action) for action in Policy]).reshape((4, 4)))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}