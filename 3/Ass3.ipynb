{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<img src=\"img/vaf1.png\">\n<img src=\"img/vaf2.png\">\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"img/vaf1.png\">\n",
    "<img src=\"img/vaf2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.02\n",
    "max_steps = 500\n",
    "episodes = 1000000\n",
    "gamma = 0.95\n",
    "epsilon_max = 1.0\n",
    "epsilon_min = 0.1\n",
    "_lambda = 0.5\n",
    "NUMBER_OF_EVAL_SIMS = 100\n",
    "EVAL_WITH_DISCOUNT = False\n",
    "\n",
    "ACTION_NO = 3\n",
    "# -1.2 is the leftest position.\n",
    "# The env starts at a location between -0.6 and -0.4 randomly.\n",
    "# The agents win when he gets to 0.5\n",
    "C_P = [-1.2, -0.6, -0.4, 0.5]\n",
    "# -0.7 is the min speed, 0.7 is the max speed.\n",
    "C_VEL = list(np.linspace(-0.7, 0.7, 8))\n",
    "N = len(C_P) * len(C_VEL)\n",
    "\n",
    "#ToDo:maybe change this\n",
    "P = len(C_P)\n",
    "V = len(C_VEL)\n",
    "shape = (P,V,ACTION_NO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_policy(epsilon,state,Q, env):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        new_action = env.action_space.sample()  # explore\n",
    "    else:\n",
    "        p,v = state\n",
    "        new_action = np.argmax(Q[p,v, :])  # exploit\n",
    "    return new_action\n",
    "\n",
    "def init_Weights():   \n",
    "    return np.zeros((N,ACTION_NO))\n",
    "\n",
    "\n",
    "def init_E(W):\n",
    "    return np.zeros((P,V))\n",
    "\n",
    "\n",
    "def init_Q():\n",
    "    return np.zeros(shape)\n",
    "\n",
    "\n",
    "def get_features(p, v):\n",
    "    \"\"\"\n",
    "    returns a vector phi that each entry is computed feature for given p, v\n",
    "    \"\"\"\n",
    "    # reshape to make it a matrix with one row (so we can transpose it later)\n",
    "    prod = product(C_P, C_VEL)\n",
    "    C = [np.array(val).reshape((1, -1)) for val in prod]\n",
    "    p_v = np.array([p, v]).reshape((1, -1)).T\n",
    "    X = np.array([p_v - c_entry.T for c_entry in C])\n",
    "    inv_cov = np.linalg.inv(np.diag([0.04, 0.0004]))\n",
    "    phi = np.array([np.e ** (-(xi.T @ inv_cov @ xi) / 2) for xi in X])\n",
    "    return np.squeeze(phi)   # get rid of 2 unnecessary dimensions\n",
    "\n",
    "def stochasticGradient(p,v,W):\n",
    "    return get_features(p,v)\n",
    "\n",
    "\n",
    "# float p, float v -> p_index,v_index\n",
    "def map_p_v(observation,env):\n",
    "    low = env.observation_space.low #2 dimension\n",
    "    high = env.observation_space.high #2 dimension\n",
    "    size = (high-low) \n",
    "\n",
    "    p = int(((observation[0]-low[0])/size[0])*P)\n",
    "    v = int(((observation[1]-low[1])/size[1])*V)\n",
    "\n",
    "    return p,v\n",
    "\n",
    "def get_Q(p,v,action,W_a):\n",
    "    return get_features(p, v) @ W_a\n",
    "\n",
    "\n",
    "def sarsa_lambda(env, episodes=episodes, max_steps=max_steps,\n",
    "                 epsilon_max=epsilon_max, epsilon_min=epsilon_min, is_decay=True, _lambda=_lambda, alpha=alpha):\n",
    "\n",
    "    # (p,v,a) = (p,v,i) @ (i,a) => the shape of W should be (i,a): (32,3) \n",
    "    W = init_Weights()\n",
    "    \n",
    "    total_steps = 0\n",
    "    epsilon = epsilon_max\n",
    "    policy_vals = []\n",
    "\n",
    "    for k in range(episodes):\n",
    "        # init E,S,A\n",
    "        E = init_E(W)\n",
    "        #state = (pos,vel)\n",
    "        state = env.reset()\n",
    "        Q =  init_Q()\n",
    "        action = eps_greedy_policy(epsilon, map_p_v(state,env), Q, env)\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Take action A, obvserve R,S'\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            #state = (pos,vel)\n",
    "            new_action = eps_greedy_policy(epsilon, map_p_v(new_state,env), Q, env)\n",
    "            p,v = new_state\n",
    "            delta_error = reward + gamma * get_Q(p,v,new_action,W[:,new_action]) - get_Q(p,v,action,W[:,action])\n",
    "            E[map_p_v(new_state,env)] += 1 # E suppose to be like W shape, so i am not sure what should represent the first dimension\n",
    "            E = np.multiply(gamma * _lambda, E) + (stochasticGradient(p,v,W)).reshape((P,V))\n",
    "            deltaW = (np.multiply(alpha*delta_error,E)).reshape(P*V)\n",
    "    \n",
    "            W[:,action]+=deltaW    \n",
    "            state = new_state\n",
    "            action = new_action\n",
    "            total_steps += 1\n",
    "\n",
    "            #ToDo: play with number of steps\n",
    "            if (total_steps < 20000 and total_steps % 2000 == 0) or (total_steps >= 20000 and total_steps % 8000 == 0):\n",
    "                policy = np.argmax(Q, axis=2)\n",
    "                policy_evaluate = policy_eval(policy, env, with_discount=EVAL_WITH_DISCOUNT)\n",
    "                show_sim_in_env(env,policy)\n",
    "                policy_vals.append((total_steps, policy_evaluate))\n",
    "                print(\"alpha:{0:.3f} _lambda:{1:.3f} starting epsilon:{2:.3f} current epsilon:{3:.3f}\".format(alpha,_lambda,epsilon_max,epsilon))\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if is_decay:\n",
    "            epsilon = epsilon_min + (epsilon_max - epsilon_min) * np.exp(-0.005 * k)\n",
    "\n",
    "        if total_steps > 1e6:\n",
    "            break\n",
    "\n",
    "    for p in range(N):\n",
    "        for v in range(N):\n",
    "            for a in range(ACTION_NO):\n",
    "                Q[p,v,a] = get_Q(p,v,action,W[a])\n",
    "\n",
    "    return Q, np.argmax(Q, axis=2), policy_vals  # returns Q, the policy and the values of the policy during the run\n",
    "\n",
    "def policy_eval(policy, env, with_discount=False):\n",
    "    \"\"\"\n",
    "    policy should be an iterable with length of number of states (action per state)\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for i in range(NUMBER_OF_EVAL_SIMS):\n",
    "        state = env.reset()\n",
    "        state = map_p_v(state,env)\n",
    "\n",
    "        run_reward = 0\n",
    "        is_done = False\n",
    "        steps = 0\n",
    "        while not is_done:\n",
    "            state, reward, is_done, _ = env.step(policy[state])\n",
    "            state = map_p_v(state,env)\n",
    "            steps += 1\n",
    "            if with_discount:\n",
    "                run_reward += reward * (gamma ** steps)\n",
    "            else:\n",
    "                run_reward += reward\n",
    "\n",
    "        rewards.append(run_reward)\n",
    "\n",
    "    return np.mean(rewards)\n",
    "\n",
    "\n",
    "def show_sim_in_env(env, policy):\n",
    "    state = env.reset()\n",
    "    state = map_p_v(state,env)\n",
    "    env.render()\n",
    "\n",
    "    total_reward = 0\n",
    "    is_done = False\n",
    "    num_of_steps = 0\n",
    "\n",
    "    while not is_done:\n",
    "        action = np.argmax(policy[state])\n",
    "        state, step_reward, is_done, _ = env.step(action)\n",
    "        state = map_p_v(state,env)\n",
    "        total_reward += step_reward\n",
    "\n",
    "        num_of_steps += 1\n",
    "        env.render()\n",
    "\n",
    "    print('done in {} steps and reward: {}'.format(num_of_steps, total_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-1.2  -0.07]\n[0.6  0.07]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env._max_episode_steps = 500\n",
    "print(env.observation_space.low)\n",
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q, policy, policy_vals = sarsa_lambda(env, episodes, max_steps, alpha=alpha, _lambda=_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}